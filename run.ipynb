{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/DLR-RM/stable-baselines3.git@40e0b9d\n",
    "!pip install gymnasium\n",
    "!git clone https://github.com/muhd-umer/rl-wireless.git\n",
    "\n",
    "# Colab path\n",
    "%cd /content/rl-wireless"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.record_episode_statistics import RecordEpisodeStatistics\n",
    "from agents import DQNAgent\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering and Testing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "global N, M, K, Ns, asd_degs, min_P, max_P, num_P, num_episodes, dtype, seed\n",
    "N = 7\n",
    "M = 32\n",
    "K = 10\n",
    "Ns = 500\n",
    "asd_degs = [\n",
    "    30,\n",
    "]\n",
    "min_P = -20\n",
    "max_P = 23\n",
    "num_P = 10\n",
    "dtype = np.float32\n",
    "seed = 0\n",
    "\n",
    "# Register and create the environment\n",
    "gym.register(id=\"MassiveMIMO-v0\", entry_point=\"network:MassiveMIMOEnv\")\n",
    "\n",
    "env = gym.make(\n",
    "    \"MassiveMIMO-v0\",\n",
    "    N=N,\n",
    "    M=M,\n",
    "    K=K,\n",
    "    Ns=Ns,\n",
    "    min_P=min_P,\n",
    "    max_P=max_P,\n",
    "    num_P=num_P,\n",
    "    dtype=dtype,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed):\n",
    "    def thunk():\n",
    "        env = gym.make(\n",
    "            env_id,\n",
    "            N=N,\n",
    "            M=M,\n",
    "            K=K,\n",
    "            Ns=Ns,\n",
    "            min_P=min_P,\n",
    "            max_P=max_P,\n",
    "            num_P=num_P,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        env = RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv([make_env(\"MassiveMIMO-v0\", seed)])\n",
    "assert isinstance(\n",
    "    envs.single_action_space, gym.spaces.Discrete\n",
    "), \"Only discrete action space is supported.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 500000\n",
    "target_freq = Ns\n",
    "\n",
    "agent = DQNAgent(envs, target_freq=target_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for global_step in range(total_timesteps):\n",
    "    agent.get_actions(global_step)\n",
    "    next_obs, b_reward, b_terminated, b_truncated, b_info = agent.envs.step(\n",
    "        agent.actions\n",
    "    )\n",
    "\n",
    "    b_done = [b_terminated[i] or b_truncated[i] for i in range(agent.envs.num_envs)]\n",
    "\n",
    "    real_next_obs = next_obs.copy()\n",
    "    for idx, d in enumerate(b_done):\n",
    "        if d:\n",
    "            real_next_obs[idx], _ = agent.envs.envs[idx].reset()\n",
    "    agent.replay_buffer.add(\n",
    "        agent.obs, real_next_obs, agent.actions, b_reward, b_done, b_info\n",
    "    )\n",
    "\n",
    "    agent.obs = next_obs\n",
    "    agent.train(global_step, start_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcsproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
